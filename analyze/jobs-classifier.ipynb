{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Fiction - Building a training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Overall goal of this notebook is to build a training model based on JOBFICTION database. We would expect to have a corpus, job classifiers, keywords dictionary and model results. All the results will be persisted and updated with the new jobs being collected.\n",
    "\n",
    "Based on the input from job seekers i.e. job descriptions submitted we will able to determine \n",
    "- job titles closest to the job description or keywords submitted (based on the weights associated)\n",
    "- closest job posts\n",
    "- keywords to search for the right job posts\n",
    "\n",
    "The first part of this notebook will explore how jobs in the JOBFICTION database can be classified. \n",
    "\n",
    "**Why do we have to classify the job posts?**\n",
    "\n",
    "A `truck driver` job post is way different from a `database administrator` job post. With the help of clustering algorithms I would expect we can categorize similar jobs into same class or cluster based purely on the job description. Similar to movie genres this classifier is expected to create job categories based on similarity of job descriptions. We can then study the job titles under the same cluster to see how true clusters. The challenge is defining the number of clusters. \n",
    "\n",
    "## Approach\n",
    "- export job descriptions, job title, company and job id from JOBFICTION database\n",
    "- tokenizing and stemming each job description\n",
    "- transforming the corpus into vector space using tf-idf\n",
    "- calculating cosine distance between each job as a measure of similarity\n",
    "- clustering the documents using the k-means algorithm\n",
    "- using multidimensional scaling to reduce dimensionality within the corpus\n",
    "- plot the clusters\n",
    "- Hierarchical clustering on the corpus using [Ward clustering](http://en.wikipedia.org/wiki/Ward%27s_method)\n",
    "- plot the clusters with hierarchial claustering\n",
    "- topic modeling using Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export data from JOBFICTION database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract jobs from JOBFICTION database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the jobs table, job description is an array of sentences. In order to export job description, this mongo javasript will be run to combine array elements as a string. For traceback we will add __id field to every record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting export_jobs_with_title.js\n"
     ]
    }
   ],
   "source": [
    "%%writefile export_jobs_with_title.js\n",
    "db.jobs.find({}, { _id: 1, jobtitle: 1, company: 1, summary: 1}).forEach( function (x) \n",
    "    {     \n",
    "        var jobdesc = '';\n",
    "        x.summary.forEach( function (y) { \n",
    "            jobdesc += y.replace(/\\n/g, ' '); \n",
    "        });     \n",
    "        print(x._id + \"!@#\" + x.jobtitle + \"!@#\" + x.company + \"!@#\" + jobdesc);\n",
    "    });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run export script to dump data to text file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "real\t0m7.431s\r\n",
      "user\t0m4.135s\r\n",
      "sys\t0m0.640s\r\n"
     ]
    }
   ],
   "source": [
    "!time mongo JOBFICTION --quiet export_jobs_with_title.js > ./data/export_jobs_w_title.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575279 /home/rt/wrk/jobs/export_jobs_w_title.txt\n",
      "indeed_2248d91cabe9f4c1!@#Records Clerk I!@#Swedish Covenant Hospital!@#To name, scan and file medical records in our EMR, eClinical Works (eCW) as directed in order to maintain the organization of the departmentâs daily operation. Back up inbound and outbound faxing needs, mail drop off and collection and to Foster Medical Pavilion post box as well as main hospital mail room.  RESPONSIBILITIES  Essential Functions   Demonstrates a commitment to the mission of Swedish Covenant Hospital and  demonstrates a service orientation and adheres to all responsibilities and standards  of the Hospital.  1. Names, scans and files medical records into patient charts in eCW following accepted naming conventions and protocols on where various medical records are filed in the chart. This is high volume work.  2. Works with incoming faxes as well as outgoing faxes ensuring they reach the appropriate provider or area following standardized protocols in the Foster Medical Pavilion Medical Records department.  3. Helps to deliver incoming mail as well as outgoing mail, whether taking to hospital main mail room or collecting mail from U.S. Post box in Foster Medical Pavilion.  4. Answers incoming phones for Medical Records and constantly checks Telephone Encounters and voice mails addressing each in a timely and appropriate fashion following standard eCW guidelines.  PATIENT CARE/AGE SPECIFIC RESPONSIBILITIES AND QUALIFICATIONS  N/A  QUALIFICATIONS/BASIC JOB REQUIREMENTS  Â·  High School Diploma or equivalent  Â·  Numerical and alphabetical organization skills  Â·  Accurate typing skills  Â·  Accurate and detail-oriented   Ability to work as part of a small team assisting all providers and teams in FMP Suite 601, 602, 603 and 604 with their medical record needs.  Job  :  Operations Support DIV-19  Primary Location  :  Foster Medical Pavillion  Organization  :  Swedish_Covenant_Mgmt_Services_Inc  Shift  Day  Hours Per Week  :  40  Job Posting  :  Feb 10, 2016, 4:56:03 PM\n"
     ]
    }
   ],
   "source": [
    "!wc -l /home/rt/wrk/jobs/export_jobs_w_title.txt\n",
    "!head -1 /home/rt/wrk/jobs/export_jobs_w_title.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just export top 100000 job descriptions (remember it's only driver related job posts) and let's build LDA model. Once LDA model is built, we will perform online training for new job posts and keep updating corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -100000 /home/rt/wrk/jobs/export_jobs_w_title.txt | awk -F'!@#' '{print $4}' > /home/rt/wrk/jobs/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -100000 /home/rt/wrk/jobs/export_jobs_w_title.txt | awk -F'!@#' 'BEGIN{OFS=\"|\"}{print $1, $2, $3}' > /home/rt/wrk/jobs/train_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indeed_2248d91cabe9f4c1|Records Clerk I|Swedish Covenant Hospital\r\n",
      "indeed_6ed966da9f33ffc1|Associate|Potbelly Sandwich Shop\r\n",
      "indeed_8c97fa9b508897d5|MAIL HANDLER ASSISTANT|United States Postal Service\r\n",
      "indeed_8a31ad4b101a0017|In-Airport Sales Representative-Midway|Stratmar Retail Services\r\n",
      "indeed_7896face148b0248|Public Aid Investigator Trainee|State of Illinois\r\n",
      "indeed_7faa4d3e581b1fbf|Installation Technician|Comcast-Xfinity\r\n",
      "indeed_971fe05d25827953|Natural Resources Advanced Specialist - Opt 2|State of Illinois\r\n",
      "indeed_7925061549fda745|Office Dispatcher|Allan E Power Plumbing Heating and Cooling\r\n",
      "indeed_676c8a0ba858e943|Customer Service Associate|Lowes Home Improvment\r\n",
      "indeed_fcae39b731c906e9|Macy's Water Tower Place, Chicago, IL: Merchandise Team Manager|Macy's\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/rt/wrk/jobs/train_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleansing Data - Stop words, Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define few functions before we take off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace forward and back slash, hyphen, underscores and other characters\n",
    "def preprocess(text):\n",
    "    return text.replace('/', ' ').replace('\\\\', ' ').replace('_', ' ').replace('-', ' ').decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a tokenizer and stemmer to returns the set of stems in the text passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # tokenize by sentence, then by word to catch any punctuations\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out tokens not containing alphanumeric\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # tokenize by sentence, then by word to catch any punctuations\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out tokens not containing alphanumeric\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z0-9]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create p_stemmer of class PorterStemmer\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "# create p_stemmer of class SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile training docs into a list\n",
    "train = [ preprocess(line) for line in open('/home/rt/wrk/jobs/train.txt', 'r') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile training labels for tracking and debugging purposes only\n",
    "train_labels = [ line.strip('\\n').split('|') for line in open('/home/rt/wrk/jobs/train_labels.txt', 'r') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indeed_2248d91cabe9f4c1', 'Records Clerk I', 'Swedish Covenant Hospital']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_stemmed = []\n",
    "vocab_tokenized = []\n",
    "\n",
    "for jobdesc in train:\n",
    "    stemmed = tokenize_and_stem(jobdesc) \n",
    "    vocab_stemmed.extend(stemmed)\n",
    "    \n",
    "    tokenized = tokenize_only(jobdesc)\n",
    "    vocab_tokenized.extend(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 29321418 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "df_vocab = pd.DataFrame({'words': vocab_tokenized}, index = vocab_stemmed)\n",
    "print 'there are ' + str(df_vocab.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              words\n",
      "to               to\n",
      "name           name\n",
      "scan           scan\n",
      "and             and\n",
      "file           file\n",
      "medic       medical\n",
      "record      records\n",
      "in               in\n",
      "our             our\n",
      "emr             emr\n",
      "eclin     eclinical\n",
      "work          works\n",
      "ecw             ecw\n",
      "as               as\n",
      "direct     directed\n",
      "in               in\n",
      "order         order\n",
      "to               to\n",
      "maintain   maintain\n",
      "the             the\n"
     ]
    }
   ],
   "source": [
    "print df_vocab.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF and Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8caa105a06be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time tfidf_matrix = tfidf_vectorizer.fit_transform(train) #fit the vectorizer to synopses'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 804\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;31m# disable defaultdict behaviour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.8, \n",
    "    max_features=200000,\n",
    "    min_df=0.2, \n",
    "    stop_words='english',\n",
    "    use_idf=True, \n",
    "    tokenizer=tokenize_and_stem, \n",
    "    ngram_range=(1, 4)\n",
    ")\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(train) #fit the vectorizer to synopses\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.014*requir + 0.014*custom + 0.012*work + 0.011*must + 0.010*driver + 0.009*deliveri + 0.009*job + 0.008*abil + 0.007*s + 0.007*time'), (1, u'0.040*driver + 0.038*pay + 0.032*truck + 0.019*offer + 0.019*transport + 0.017*paid + 0.017*compani + 0.016*weekli + 0.016*drive + 0.015*cpm'), (2, u'0.037*driver + 0.017*team + 0.016*year + 0.014*truck + 0.014*per + 0.014*mile + 0.011*paid + 0.010*compani + 0.010*000 + 0.009*pay'), (3, u'0.040*truck + 0.035*driver + 0.019*pay + 0.018*roehl + 0.015*ll + 0.014*get + 0.013*drive + 0.013*home + 0.013*cdl + 0.013*train'), (4, u'0.021*haul + 0.020*oper + 0.018*owner + 0.015*move + 0.015*graebel + 0.014*high + 0.014*ca + 0.011*revenu + 0.011*long + 0.011*opportun'), (5, u'0.022*oper + 0.020*owner + 0.018*program + 0.016*knight + 0.015*driver + 0.015*year + 0.014*fuel + 0.013*truck + 0.013*leas + 0.011*busi'), (6, u'0.045*driver + 0.041*pay + 0.026*mile + 0.024*barr + 0.024*nunn + 0.021*per + 0.019*truck + 0.018*cdl + 0.018*benefit + 0.017*hire'), (7, u'0.032*driver + 0.020*drive + 0.017*compani + 0.014*hazmat + 0.014*year + 0.013*benefit + 0.013*truck + 0.013*class + 0.013*polici + 0.012*cdl'), (8, u'0.023*year + 0.019*driver + 0.016*cdl + 0.015*month + 0.015*class + 0.015*call + 0.013*drive + 0.013*home + 0.012*3 + 0.012*pay'), (9, u'0.036*driver + 0.022*pay + 0.018*compani + 0.016*flatb + 0.014*offer + 0.013*s + 0.012*now + 0.012*werner + 0.012*benefit + 0.012*opportun')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create english stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "    \n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.014*requir + 0.014*custom + 0.012*work + 0.011*must + 0.010*driver\n",
      "1 0.040*driver + 0.038*pay + 0.032*truck + 0.019*offer + 0.019*transport\n",
      "2 0.037*driver + 0.017*team + 0.016*year + 0.014*truck + 0.014*per\n",
      "3 0.040*truck + 0.035*driver + 0.019*pay + 0.018*roehl + 0.015*ll\n",
      "4 0.021*haul + 0.020*oper + 0.018*owner + 0.015*move + 0.015*graebel\n",
      "5 0.022*oper + 0.020*owner + 0.018*program + 0.016*knight + 0.015*driver\n",
      "6 0.045*driver + 0.041*pay + 0.026*mile + 0.024*barr + 0.024*nunn\n",
      "7 0.032*driver + 0.020*drive + 0.017*compani + 0.014*hazmat + 0.014*year\n",
      "8 0.023*year + 0.019*driver + 0.016*cdl + 0.015*month + 0.015*class\n",
      "9 0.036*driver + 0.022*pay + 0.018*compani + 0.016*flatb + 0.014*offer\n"
     ]
    }
   ],
   "source": [
    "for topic, keywords in ldamodel.print_topics(num_topics=10, num_words=5):\n",
    "    print topic, keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Let's TEST the LDA model with new job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Hiring Company Truck Drivers. At Transport America We Raised Pay!  Company Truck Driver Benefits: Top 10% Industry Pay Year-Round Steady Freight Performance Pay - Experienced Drivers Earn Top Scale in 2 Years Flexible Home Time, Including Get Home Certificates 24/7 Support, 365 Days A Year Pick Your Schedule Option Lease Purchase Options Day 1 Medical/Dental/Vision/Disability Benefits Package Transfer Opportunities Available E-Logs and an InCab Communication Hub Roll Stability and OnGuard System CSA Safe Carrier New Fleet of Equipment- New Kenworths In Delivery  At Transport America, our goal is to deliver excellence in all that we do. At a time when others are moving to asset lite models, we are committed to running assets in networks, which gives you reliable capacity with an excellence of service unsurpassed in the transportation industry. We are big enough to create meaningful solutions, but small enough to provide you the level of customer service you deserve. We believe in hiring the best truck drivers in the industry and empower them to create solutions for our customers. Because of our asset intensity, we attract and retain the best drivers in the trucking industry. The technology we employ is focused on enhancing your service experience. Our experienced driver base, with retention levels well above the industry average, sets us apart from our competitors. Transport America's fleet of company truck drivers is the best and most experienced on the road. We welcome you to fill out the form above to be contacted by one of our recruiters! Call us for details at 877-957-3117\r\n"
     ]
    }
   ],
   "source": [
    "!tail -1 ./data/export.txt | awk -F'!@#' '{print $2}' > ./data/test.txt\n",
    "!cat ./data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile sample documents into a list\n",
    "test_set = [ preprocess(line) for line in open('./data/test.txt', 'r') ]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in test_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "bow = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's print topics from the model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.014*requir + 0.014*custom + 0.012*work + 0.011*must + 0.010*driver\n",
      "1 0.040*driver + 0.038*pay + 0.032*truck + 0.019*offer + 0.019*transport\n",
      "2 0.037*driver + 0.017*team + 0.016*year + 0.014*truck + 0.014*per\n",
      "3 0.040*truck + 0.035*driver + 0.019*pay + 0.018*roehl + 0.015*ll\n",
      "4 0.021*haul + 0.020*oper + 0.018*owner + 0.015*move + 0.015*graebel\n",
      "5 0.022*oper + 0.020*owner + 0.018*program + 0.016*knight + 0.015*driver\n",
      "6 0.045*driver + 0.041*pay + 0.026*mile + 0.024*barr + 0.024*nunn\n",
      "7 0.032*driver + 0.020*drive + 0.017*compani + 0.014*hazmat + 0.014*year\n",
      "8 0.023*year + 0.019*driver + 0.016*cdl + 0.015*month + 0.015*class\n",
      "9 0.036*driver + 0.022*pay + 0.018*compani + 0.016*flatb + 0.014*offer\n"
     ]
    }
   ],
   "source": [
    "for topic, keywords in ldamodel.print_topics(num_topics=10, num_words=5):\n",
    "    print topic, keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's see what topis test document belongs to **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.85813606763953532), (2, 0.038124342565039944), (3, 0.052171292370160209), (7, 0.011156941430638388), (9, 0.03004651314823252)]\n"
     ]
    }
   ],
   "source": [
    "for topics in ldamodel[bow]:\n",
    "    print topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** So the test document belongs to topics 0, 2, 3, 7 and 9 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Now Hiring Company Truck Drivers. At Transport America We Raised Pay!  Company Truck Driver Benefits: Top 10% Industry Pay Year Round Steady Freight Performance Pay   Experienced Drivers Earn Top Scale in 2 Years Flexible Home Time, Including Get Home Certificates 24 7 Support, 365 Days A Year Pick Your Schedule Option Lease Purchase Options Day 1 Medical Dental Vision Disability Benefits Package Transfer Opportunities Available E Logs and an InCab Communication Hub Roll Stability and OnGuard System CSA Safe Carrier New Fleet of Equipment  New Kenworths In Delivery  At Transport America, our goal is to deliver excellence in all that we do. At a time when others are moving to asset lite models, we are committed to running assets in networks, which gives you reliable capacity with an excellence of service unsurpassed in the transportation industry. We are big enough to create meaningful solutions, but small enough to provide you the level of customer service you deserve. We believe in hiring the best truck drivers in the industry and empower them to create solutions for our customers. Because of our asset intensity, we attract and retain the best drivers in the trucking industry. The technology we employ is focused on enhancing your service experience. Our experienced driver base, with retention levels well above the industry average, sets us apart from our competitors. Transport America's fleet of company truck drivers is the best and most experienced on the road. We welcome you to fill out the form above to be contacted by one of our recruiters! Call us for details at 877 957 3117\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print test_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
